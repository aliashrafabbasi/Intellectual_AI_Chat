# Intellectual AI Chat ğŸ¤–

A production-ready AI chat backend built with **FastAPI**, **MongoDB**, and **Groq LLM**.
Supports chat memory, summarization, and scalable API design.

---

## ğŸš€ Features
- FastAPI REST APIs
- Chat memory (MongoDB)
- LLM integration (Groq)
- Clean layered architecture
- Ready for production deployment

---

## ğŸ§  Tech Stack
- Python 3.12
- FastAPI
- MongoDB
- LangChain + Groq
- Uvicorn

---

## ğŸ“‚ Project Structure
app/
â”œâ”€â”€ api/v1 # Routes
â”œâ”€â”€ services # Business logic
â”œâ”€â”€ db # Mongo connection & queries
â”œâ”€â”€ llm # AI clients & prompts
â”œâ”€â”€ models # Pydantic models
â””â”€â”€ core # Config & constants
---

## âš™ï¸ Setup

```bash
git clone git@github.com:aliashrafabbasi/Intellectual_AI_Chat.git
cd Intellectual_AI_Chat
python -m venv myenv
source myenv/bin/activate
pip install -r requirements.txt
Create .env:

MONGO_URI=your_mongo_uri
GROQ_API_KEY=your_groq_key


Run server:

uvicorn main:app --reload

ğŸ“¡ API Docs

Visit:
ğŸ‘‰ http://127.0.0.1:8000/docs

ğŸ§ª Example Endpoint
POST /api/v1/chat?session_id=abc123&message=Hello

ğŸ“Œ Status

âœ… Chat memory
âœ… Summarization
ğŸ”œ Auth & rate limiting

ğŸ§‘â€ğŸ’» Author

Ali Ashraf Abbasi


ğŸ‘‰ Is README ko paste karo, commit & push later.

---

# âœ… 2ï¸âƒ£ MongoDB Indexes (VERY IMPORTANT ğŸ”¥)

### âŒ Problem (without indexes)
- Chat history slow
- Memory retrieval expensive
- Scale pe lag

### âœ… Solution: indexes

Open Mongo shell / Compass and run:

```js
db.chats.createIndex({ session_id: 1 })
db.chats.createIndex({ created_at: -1 })


Agar messages embedded hain:

db.chats.createIndex({ "messages.timestamp": -1 })

ğŸ¯ Benefit

Fast chat recall

Summarization smooth

Production-safe

âœ… 3ï¸âƒ£ Production Tips (Real-world)
ğŸ” Environment

.env NEVER commit (already correct âœ…)

Use dotenv only locally

Production me env vars system se

ğŸš¦ Rate Limiting (next logical step)

Add later:

slowapi or

API Gateway (NGINX / Cloudflare)

âš¡ Uvicorn (Production)

Never use --reload in prod:

uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4

ğŸ§  Memory Control

You already solved:

context window limit

summarization

Next upgrade:

per-user memory limit

TTL index (auto delete old chats)

db.chats.createIndex({ created_at: 1 }, { expireAfterSeconds: 2592000 })


(30 days auto cleanup)

ğŸ Verdict

This project is now:

âŒ NOT tutorial-level

âœ… REAL backend engineer level

âœ… Resume + portfolio worthy

âœ… Extendable to SaaS
